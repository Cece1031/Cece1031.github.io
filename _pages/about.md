---
permalink: /
title: "Qingni Wang | Homepage"
excerpt: "Master's student in Computer Science, specializing in Uncertainty Quantification and Multimodal LLMs."
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

## About Me

I am **Qingni Wang**, a Master's student in Computer Science at the **University of Electronic Science and Technology of China (UESTC)**. My research interests include:
- **Uncertainty Quantification:** Conformal Prediction, Conformal Risk Control.
- **Hallucination in Question Answering (QA) Tasks of Multimodal Large Language Models (MLLMs).**
You can find my CV here: [Qingni's CV](../assets/wqn_phd.pdf).
You can check out my publications and updates on my [Google Scholar profile](https://scholar.google.com/citations?user=awhNfL4AAAAJ). 

# üî• News
- *2024.11*: üéâüéâ One paper **"ConU: Conformal Uncertainty in Large Language Models with Correctness Coverage Guarantees."** has been accepted to **EMNLP 2024 (Findings)**.
- *2025.01*: üéâüéâ One paper **"Sample then Identify: A General Framework for Risk Control and Assessment in Multimodal Large Language Models"** has been accepted to **ICLR 2025 Spotlight (5%)**.
- *2025.02*: üéâüéâ One paper **"LongVALE: Vision-Audio-Language-Event Benchmark Towards Time-Aware Omni-Modal Perception of Long Videos."** has been accepted to **CVPR 2025**.

# üìù Publications

### Conference Papers:
1. **Qingni Wang**, Tiantian Geng, Zhiyuan Wang, Teng Wang, Bo Fu*, Feng Zheng*. "Sample then Identify: A General Framework for Risk Control and Assessment in Multimodal Large Language Models." **ICLR 2025 (Spotlight).** [[Paper]](https://openreview.net/forum?id=9WYMDgxDac)
2. **Tiantian Geng, Jinrui Zhang, Qingni Wang**, Teng Wang, Jinming Duan*, Feng Zheng*. "LongVALE: Vision-Audio-Language-Event Benchmark Towards Time-Aware Omni-Modal Perception of Long Videos." **CVPR 2025.**[[Paper]](https://arxiv.org/pdf/2411.19772)
3. **Zhiyuan Wang, Jinhao Duan, Lu Cheng, Yue Zhang, Qingni Wang**, Xiaoshuang Shi*, Kaidi Xu, Hengtao Shen, Xiaofeng Zhu. "ConU: Conformal Uncertainty in Large Language Models with Correctness Coverage Guarantees." **EMNLP 2024 (Findings).** [[Paper]](https://aclanthology.org/2024.findings-emnlp.404/)

# üéñ Honors and Awards
- **Academic Seedling Award**, 2025
- **Outstanding Student Scholarship**, 2023, 2024
- **First Prize Scholarship**, 2020, 2021

# üìñ Education
- **China University of Mining and Technology (CUMT)** (2019.09 - 2023.06)
  - Bachelor‚Äôs degree in Electronic Information Science and Technology
- **University of Electronic Science and Technology of China (UESTC)** (2023.09 - Present)
  - Master's degree in Computer Science and Technology

# üí¨ Research Experience
### A General Framework for Risk Control and Assessment in Multimodal LLMs (ICLR 2025)
- Developed **TRON**, a two-step risk management framework for VideoQA tasks in MLLMs.
- Proposed a **Sampling Step** (conformal score calibration) and an **Identification Step** (non-conformity score based on self-consistency theory).
- Achieved rigorous guarantees of marginal coverage and minimum sampling size in open-domain QA tasks.

### LongVALE: Vision-Audio-Language-Event Benchmark (CVPR 2025)
- Conducted comprehensive evaluation of various MLLMs on the **LongVALE benchmark dataset**.
- Analyzed model performance across different metrics, identifying strengths and weaknesses in handling multimodal tasks.

# üì© Contact
**Email:** qingni1031@gmail.com
